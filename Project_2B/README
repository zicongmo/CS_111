QUESTION 2.3.1
1. I would guess that for low thread counts, most of the CPU cycles are spent executing the critical sections of the code, since the probability of lock contention is so low.
2. Because the probability that another thread has the lock is low when the number of threads is small, there would not be many CPU cycles wasted trying to obtain the lock. Instead, most of the CPU cycles can be spent doing meaningful work in the critical sections of the code.
3. I would guess that for high thread counts, most of the CPU cycles in the spin-lock tests are spent spinning around doing nothing waiting for the lock to become available. As the number of threads increases, the probability of contention increases, so a lot more time is spent spinning.
4. I would guess that most of the CPU cycles in the mutex tests are spent setting up for context switches and performing system calls, since this is how mutexes behave when a lock is occupied. 

QUESTION 2.3.2
1. The lines of code that are consuming most of the cycles are those that acquire the lock (while(___sync_lock_test_and_set)). 
2. Because the probability that a given lock is already being held is so high when the number of threads is large, a thread will likely have to wait for a lock to become available. Because we are using spin locks, the thread will spin and waste CPU cycles, which is why it becomes so expensive to wait for spin locks with a large number of threads.

QUESTION 2.3.3
1. Because the probability that a given lock is already being held is so high for a large number of threads, it is unlikely that a lock is available when a thread needs it. Therefore, the thread must wait for the lock, and as the number of other threads increases, the longer the thread has to wait for the lock, since there are lots of other threads waiting for the same lock. 
2. Because the fact that threads are waiting for locks means that there is a thread currently using the lock, there is always some thread making progress on an operation. Therefore, the amount of time per operation isn't affected that heavily by the amount of time spent waiting for locks.
3. Because each thread is calculating the time it spends waiting on a lock, and this is done in parallel, the same amount of time is added multiple times. For example, if two threads each wait two seconds in parallel, the actual amount of elapsed time is two seconds. This number is what's reported by the main thread. However, if we sum up the amount of time each thread reports that it waited, we get four seconds, which is what's represented by average wait time. Therefore, as the number of threads waiting for locks increases, the amount of total time spent waiting on locks increases, so the average time also increases. In contrast, there is always some thread making progress on an operation, so this quantity doesn't increase as quickly. 

QUESTION 2.3.4
1. As the number of lists increases, we see that the number of operations performed per second increases for both the mutex and the spin lock case. This is because as the number of lists increases, the number of locks also increases, so the amount of time spent waiting on a lock decreases. Since the wasted time decreases, we can spend more time doing meaningful work, so throughput increases.
2. The throughput should continue increasing, but will approach some limiting value. As the number of lists increases, the number of locks increases, until the probability of contention is zero. At this point, the maximum throughput will be achieved.
3. Although the logarithmic nature of the graph makes it difficult to determine exactly, it does seem that the four-list case is about four times faster than the single list case. It does seem true that the throughput of an N-way partitioned list is equivalent to the throughput of a single list with fewer threads, based on my graphs.

NOTE: The graphs do not always correlate exactly with the answers I have above. Because the shape of the graph relies heavily on the timing, sometimes the server slowing down/speeding up and greatly affect the shape of the graph. The answers I have above are based on what I saw overall over multiple iterations of the graphs. In particular, graphs 4 and 5 should have a smoother shape, and the curves should not overlap. 

CONTENTS:
lab2b-804654167.tar.gz: Tarball containing source files, Makefile, README, csv files, profiler, and script used to generate the plots.
Makefile: Provides easy way to build the following targets:
	default: Builds the lab2b_list executable
	tests: Runs 100 test cases, storing output data into lab2b_list.csv
	graphs: Uses the data in the csv files to generate .png files showing timing relationships.
	dist: Builds the tarball
	clean: Removes files generated by the Makefile
README: Provides answers to the selected questions, and a description of the contents of the tarball.
profile.out: Text file showing the amount of time spent in each section of the code.
lab2b_list.c: Source file used to build the lab2b_list executable
SortedList.h: Header file describing the functions used to manipulate sorted lists
SortedList.c: Source file implementing the functions described in SortedList.h
lab2b_list.csv: csv file containing data for each test done on lab2b_list
lab2b_list.gp: Script using gnuplot to generate .png files using the data in the csv
lab2b_1.png: Graph showing the number of operations per second as a function of thread count
lab2b_2.png: Graph showing the amount of time spent waiting on locks and average time per operation as a function of thread count
lab2b_3.png: Graph showing which combinations of thread count and iterations lead to a successful run for each of the synchronization options
lab2b_4.png: Graph showing the number of operations per second as a function of thread count for mutexes, for four different list counts
lab2b_4.png: Graph showing the number of operations per second as a function of thread count for spin locks, for four different list counts
